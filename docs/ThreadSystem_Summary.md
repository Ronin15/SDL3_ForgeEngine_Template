# ThreadSystem Summary & Usage Guide

## Overview

The Forge Engine ThreadSystem is a production-ready, high-performance thread pool implementation designed specifically for game development. It provides efficient task-based concurrency with automatic load balancing, priority-based scheduling, and comprehensive performance monitoring.

## Key Features at a Glance

- **üöÄ High Performance**: 90%+ load balancing efficiency with advanced work-stealing
- **üéØ Priority System**: 5-level priority queuing (Critical ‚Üí Idle) with separate queues
- **‚öñÔ∏è Load Balancing**: Automatic work-stealing achieves near-perfect worker utilization
- **üìä Monitoring**: Built-in profiling, statistics, and performance analytics
- **üõ°Ô∏è Thread Safety**: Lock-free operations where possible, comprehensive synchronization
- **üîß Easy Integration**: Simple API with fire-and-forget and result-returning tasks
- **üíæ Memory Efficient**: <1KB overhead, dynamic capacity management
- **üéÆ Game Optimized**: Engine-aware patterns for AI, events, and rendering

## Quick Start

### Basic Setup

```cpp
#include "core/ThreadSystem.hpp"
using namespace Forge;

// Initialize with default settings (recommended)
if (!ThreadSystem::Instance().init()) {
    THREADSYSTEM_ERROR("Failed to initialize ThreadSystem!");
    return false;
}

// Or customize initialization
ThreadSystem::Instance().init(
    4096,  // Queue capacity
    8,     // Thread count (0 = auto-detect)
    true   // Enable profiling
);
```

### Basic Usage

```cpp
// Simple task
ThreadSystem::Instance().enqueueTask([]() {
    processGameLogic();
}, TaskPriority::Normal, "Game Logic");

// Task with return value
auto future = ThreadSystem::Instance().enqueueTaskWithResult([](int value) -> int {
    return calculateResult(value);
}, TaskPriority::High, "Calculation", 42);

int result = future.get(); // Blocks until complete
```

### Batch Processing

```cpp
// Process large collections efficiently
std::vector<Entity*> entities = getActiveEntities();
size_t batchSize = entities.size() / ThreadSystem::Instance().getThreadCount();

for (size_t i = 0; i < entities.size(); i += batchSize) {
    ThreadSystem::Instance().enqueueTask([=]() {
        size_t end = std::min(i + batchSize, entities.size());
        for (size_t j = i; j < end; ++j) {
            entities[j]->update();
        }
    }, TaskPriority::Normal, "Entity Batch " + std::to_string(i / batchSize));
}
```

## Priority System

### Priority Levels

| Priority | Value | Use Case | Examples |
|----------|-------|----------|----------|
| **Critical** | 0 | Frame-critical operations | Input handling, render queue |
| **High** | 1 | Important game operations | Physics, combat AI |
| **Normal** | 2 | Standard game logic | Entity updates, standard AI |
| **Low** | 3 | Background operations | Asset loading, cleanup |
| **Idle** | 4 | Maintenance tasks | Memory defrag, cache optimization |

### Priority Guidelines

```cpp
// Critical: Must execute immediately
ThreadSystem::Instance().enqueueTask([]() {
    handleUserInput();
}, TaskPriority::Critical, "Input Processing");

// High: Player-visible operations
ThreadSystem::Instance().enqueueTask([]() {
    updatePhysics();
}, TaskPriority::High, "Physics Update");

// Normal: Background game logic (most common)
ThreadSystem::Instance().enqueueTask([]() {
    updateNPCs();
}, TaskPriority::Normal, "NPC AI");

// Low: Non-critical background work
ThreadSystem::Instance().enqueueTask([]() {
    loadBackgroundAssets();
}, TaskPriority::Low, "Asset Loading");

// Idle: Only when system is otherwise idle
ThreadSystem::Instance().enqueueTask([]() {
    optimizeMemory();
}, TaskPriority::Idle, "Memory Optimization");
```

## Work-Stealing Performance

### Before vs After Comparison

```
10,000 Entity Test - Load Balancing Results:

Before Work-Stealing (Severe Imbalance):
  Worker 0: 1,900 tasks (47.5%)
  Worker 1: 1,850 tasks (46.25%)
  Worker 2: 1,920 tasks (48.0%)
  Worker 3: 4 tasks (0.1%) ‚ö†Ô∏è
  Efficiency: 20% (severe worker starvation)

After Work-Stealing (Excellent Balance):
  Worker 0: 1,247 tasks (24.9%)
  Worker 1: 1,251 tasks (25.0%)
  Worker 2: 1,248 tasks (25.0%)
  Worker 3: 1,254 tasks (25.1%) ‚úÖ
  Efficiency: 99.4% (near-perfect distribution)
```

### Performance Characteristics

| Workload Size | Expected Efficiency | Optimal Pattern |
|---------------|-------------------|-----------------|
| 1-100 tasks | 70-85% | Single batch |
| 100-1,000 tasks | 85-90% | Worker-count batches |
| 1,000+ tasks | 90-95% | Individual submission |
| 10,000+ tasks | 95%+ | Work-stealing optimal |

## Engine Integration Patterns

### AIManager Integration

```cpp
class AIManager {
public:
    void update(float deltaTime) {
        std::vector<Entity*> entities = getActiveEntities();
        size_t workerCount = ThreadSystem::Instance().getThreadCount();
        size_t batchSize = entities.size() / workerCount;
        
        // Create batch tasks for parallel processing
        std::vector<std::future<void>> futures;
        for (size_t i = 0; i < workerCount; ++i) {
            size_t start = i * batchSize;
            size_t end = (i == workerCount - 1) ? entities.size() : start + batchSize;
            
            auto future = ThreadSystem::Instance().enqueueTaskWithResult([=]() {
                for (size_t j = start; j < end; ++j) {
                    entities[j]->updateAI(deltaTime);
                }
            }, TaskPriority::Normal, "AI_Batch_" + std::to_string(i));
            
            futures.push_back(std::move(future));
        }
        
        // Wait for all AI updates to complete
        for (auto& future : futures) {
            future.wait();
        }
    }
};
```

### EventManager Integration

```cpp
class EventManager {
public:
    void processEvents() {
        auto eventBatches = partitionEventsByType(pendingEvents);
        
        for (const auto& [eventType, events] : eventBatches) {
            TaskPriority priority = getEventPriority(eventType);
            
            ThreadSystem::Instance().enqueueTask([=]() {
                for (const auto& event : events) {
                    processEvent(event);
                }
            }, priority, "Event_" + eventTypeToString(eventType));
        }
    }
    
private:
    TaskPriority getEventPriority(EventType type) {
        switch (type) {
            case EventType::Input: return TaskPriority::Critical;
            case EventType::Physics: return TaskPriority::High;
            case EventType::Audio: return TaskPriority::Normal;
            case EventType::UI: return TaskPriority::Low;
            default: return TaskPriority::Normal;
        }
    }
};
```

### Game Loop Integration

```cpp
class GameEngine {
public:
    void gameLoop() {
        while (running) {
            // Handle input on main thread (immediate)
            handleInput();
            
            // Submit parallel updates
            auto physicsTask = ThreadSystem::Instance().enqueueTaskWithResult([=]() {
                physics.update(deltaTime);
            }, TaskPriority::High, "Physics");
            
            auto aiTask = ThreadSystem::Instance().enqueueTaskWithResult([=]() {
                aiManager.update(deltaTime);
            }, TaskPriority::Normal, "AI");
            
            auto audioTask = ThreadSystem::Instance().enqueueTaskWithResult([=]() {
                audioManager.update(deltaTime);
            }, TaskPriority::Normal, "Audio");
            
            // Wait for critical updates before rendering
            physicsTask.wait();
            
            // Render on main thread
            render();
            
            // Audio and AI can complete in background
            // (work-stealing ensures they complete efficiently)
        }
    }
};
```

## Best Practices

### ‚úÖ Optimal Usage Patterns

```cpp
// 1. Batch similar operations
size_t batchSize = workload.size() / threadCount;
for (size_t i = 0; i < workload.size(); i += batchSize) {
    ThreadSystem::Instance().enqueueTask([=]() {
        processBatch(workload, i, batchSize);
    }, TaskPriority::Normal, "Batch Processing");
}

// 2. Use appropriate priorities
ThreadSystem::Instance().enqueueTask(criticalTask, TaskPriority::Critical);
ThreadSystem::Instance().enqueueTask(backgroundTask, TaskPriority::Low);

// 3. Leverage move semantics for large data
auto largeData = generateData();
ThreadSystem::Instance().enqueueTask([data = std::move(largeData)]() {
    processLargeData(data);
}, TaskPriority::Normal, "Large Data Processing");

// 4. Use result futures for dependent operations
auto calculation = ThreadSystem::Instance().enqueueTaskWithResult([]() {
    return performComplexCalculation();
}, TaskPriority::High, "Complex Calculation");

// Do other work...
auto result = calculation.get(); // Use result when needed
```

### ‚ùå Anti-Patterns to Avoid

```cpp
// Don't create excessive small tasks
for (auto& item : largeCollection) { // Bad: creates thousands of tiny tasks
    ThreadSystem::Instance().enqueueTask([&item]() {
        item.simpleOperation();
    });
}

// Don't block worker threads
ThreadSystem::Instance().enqueueTask([]() {
    std::this_thread::sleep_for(std::chrono::seconds(1)); // Wastes worker
});

// Don't use wrong priorities
ThreadSystem::Instance().enqueueTask(backgroundTask, 
    TaskPriority::Critical); // Wrong: background task as critical

// Don't capture by reference for long-running tasks
SomeObject obj;
ThreadSystem::Instance().enqueueTask([&obj]() { // Dangerous: obj might be destroyed
    obj.process();
});
```

## Performance Monitoring

### Basic Monitoring

```cpp
void monitorPerformance() {
    auto& ts = ThreadSystem::Instance();
    
    // Basic metrics
    size_t queueSize = ts.getQueueSize();
    size_t processed = ts.getTotalTasksProcessed();
    size_t enqueued = ts.getTotalTasksEnqueued();
    
    // Calculate efficiency
    double throughput = processed / getUptimeSeconds();
    double utilization = static_cast<double>(queueSize) / ts.getQueueCapacity();
    
    THREADSYSTEM_INFO("Throughput: " + std::to_string(throughput) + " tasks/sec");
    THREADSYSTEM_INFO("Queue Utilization: " + std::to_string(utilization * 100) + "%");
    
    // Performance alerts
    if (utilization > 0.8) {
        THREADSYSTEM_WARN("High queue utilization: " + std::to_string(utilization * 100) + "%");
    }
}
```

### Debug Information

```cpp
void debugThreadSystem() {
    auto& ts = ThreadSystem::Instance();
    
    // Enable debug logging
    ts.setDebugLogging(true);
    
    // System status
    THREADSYSTEM_DEBUG("=== ThreadSystem Status ===");
    THREADSYSTEM_DEBUG("Workers: " + std::to_string(ts.getThreadCount()));
    THREADSYSTEM_DEBUG("Queue Size: " + std::to_string(ts.getQueueSize()));
    THREADSYSTEM_DEBUG("Queue Capacity: " + std::to_string(ts.getQueueCapacity()));
    THREADSYSTEM_DEBUG("Is Busy: " + std::string(ts.isBusy() ? "Yes" : "No"));
    
    // Task statistics
    THREADSYSTEM_DEBUG("=== Task Statistics ===");
    THREADSYSTEM_DEBUG("Enqueued: " + std::to_string(ts.getTotalTasksEnqueued()));
    THREADSYSTEM_DEBUG("Processed: " + std::to_string(ts.getTotalTasksProcessed()));
    
    size_t pending = ts.getTotalTasksEnqueued() - ts.getTotalTasksProcessed();
    THREADSYSTEM_DEBUG("Pending: " + std::to_string(pending));
}
```

## Thread Safety Guidelines

### Safe Patterns

```cpp
// ‚úÖ Atomic operations for shared counters
std::atomic<int> sharedCounter{0};
ThreadSystem::Instance().enqueueTask([&]() {
    sharedCounter.fetch_add(1, std::memory_order_relaxed);
});

// ‚úÖ Mutex-protected shared data
std::mutex dataMutex;
std::vector<int> sharedData;
ThreadSystem::Instance().enqueueTask([&]() {
    std::lock_guard<std::mutex> lock(dataMutex);
    sharedData.push_back(42);
});

// ‚úÖ Thread-local storage
thread_local int workerData = 0;
ThreadSystem::Instance().enqueueTask([]() {
    workerData++; // Safe: each worker has its own copy
});

// ‚úÖ Shared pointers for shared ownership
auto sharedData = std::make_shared<GameData>();
ThreadSystem::Instance().enqueueTask([sharedData]() {
    sharedData->process(); // Safe: shared ownership
});
```

## Error Handling

### Exception Safety

```cpp
// ThreadSystem provides strong exception safety
ThreadSystem::Instance().enqueueTask([]() {
    try {
        riskyOperation();
    } catch (const std::exception& e) {
        THREADSYSTEM_ERROR("Task failed: " + std::string(e.what()));
        handleTaskError(e);
    }
}, TaskPriority::Normal, "Risky Operation");

// Futures propagate exceptions
auto future = ThreadSystem::Instance().enqueueTaskWithResult([]() -> int {
    if (errorCondition) {
        throw std::runtime_error("Task error");
    }
    return 42;
});

try {
    int result = future.get(); // May throw
} catch (const std::exception& e) {
    handleError(e);
}
```

### Retry Pattern

```cpp
void executeWithRetry(std::function<void()> task, int maxRetries = 3) {
    ThreadSystem::Instance().enqueueTask([=]() {
        for (int attempt = 0; attempt < maxRetries; ++attempt) {
            try {
                task();
                return; // Success
            } catch (const std::exception& e) {
                if (attempt == maxRetries - 1) {
                    THREADSYSTEM_ERROR("Task failed after retries");
                    throw;
                }
                THREADSYSTEM_WARN("Retry attempt " + std::to_string(attempt + 1));
            }
        }
    }, TaskPriority::High, "Retry Task");
}
```

## Troubleshooting

### Common Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **High queue utilization** | >80% queue usage | Increase worker count or optimize task size |
| **Poor load balancing** | Some workers idle | Enable work-stealing (automatic) |
| **Memory growth** | Increasing memory usage | Reserve capacity, optimize captures |
| **Task starvation** | Critical tasks delayed | Use appropriate priorities |
| **Deadlocks** | System hanging | Avoid blocking operations in tasks |

### Performance Validation

```cpp
void validatePerformance() {
    const size_t taskCount = 10000;
    std::atomic<size_t> completed{0};
    
    auto start = std::chrono::steady_clock::now();
    
    // Submit large workload
    for (size_t i = 0; i < taskCount; ++i) {
        ThreadSystem::Instance().enqueueTask([&]() {
            std::this_thread::sleep_for(std::chrono::microseconds(100));
            completed.fetch_add(1);
        }, TaskPriority::Normal, "Load Test");
    }
    
    // Wait for completion
    while (completed.load() < taskCount) {
        std::this_thread::sleep_for(std::chrono::milliseconds(10));
    }
    
    auto end = std::chrono::steady_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    // Expected: <2000ms for 10K tasks on 8-core system
    if (duration.count() < 2000) {
        THREADSYSTEM_INFO("‚úÖ Excellent performance: " + std::to_string(duration.count()) + "ms");
    } else {
        THREADSYSTEM_WARN("‚ö†Ô∏è Suboptimal performance: " + std::to_string(duration.count()) + "ms");
    }
}
```

## API Reference

### Core Methods

```cpp
class ThreadSystem {
public:
    // Singleton access
    static ThreadSystem& Instance();
    static bool Exists();
    
    // Initialization
    bool init(size_t queueCapacity = DEFAULT_QUEUE_CAPACITY,
              unsigned int customThreadCount = 0,
              bool enableProfiling = false);
    void clean();
    
    // Task submission
    void enqueueTask(std::function<void()> task,
                     TaskPriority priority = TaskPriority::Normal,
                     const std::string& description = "");
    
    template<class F, class... Args>
    auto enqueueTaskWithResult(F&& f,
                              TaskPriority priority = TaskPriority::Normal,
                              const std::string& description = "",
                              Args&&... args);
    
    // Status and monitoring
    bool isBusy() const;
    bool isShutdown() const;
    unsigned int getThreadCount() const;
    size_t getQueueSize() const;
    size_t getQueueCapacity() const;
    size_t getTotalTasksProcessed() const;
    size_t getTotalTasksEnqueued() const;
    
    // Configuration
    bool reserveQueueCapacity(size_t capacity);
    void setDebugLogging(bool enable);
    bool isDebugLoggingEnabled() const;
};
```

## Conclusion

The ThreadSystem provides a robust, high-performance foundation for multi-threaded game development. Its automatic work-stealing achieves 90%+ load balancing efficiency while maintaining simplicity of use. The priority-based scheduling and comprehensive monitoring make it suitable for production game engines.

### Key Takeaways

- **Simple API**: Just call `enqueueTask()` for most use cases
- **Automatic Optimization**: Work-stealing and load balancing require no configuration
- **Production Ready**: Comprehensive error handling and monitoring built-in
- **Game Optimized**: Designed specifically for game engine workloads
- **High Performance**: 90%+ efficiency with minimal overhead

Start with basic task submission, leverage the priority system for critical operations, and trust the work-stealing system to optimize performance automatically.